"""
è”é‚¦å­¦ä¹ è®­ç»ƒæ¨¡å— - å®Œæ•´ç‰ˆæœ¬

å®ç°äº†è”é‚¦å­¦ä¹ çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒï¼ˆæ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œï¼‰
- æ¨¡å‹æƒé‡èšåˆï¼ˆFedAvgï¼‰
- å…¨å±€æ¨¡å‹è¯„ä¼°ï¼ˆåŒ…å«è¯¦ç»†æŒ‡æ ‡å’Œå¯è§†åŒ–ï¼‰
- æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
- Richè¿›åº¦æ¡æ˜¾ç¤º
"""

import copy
import os
import time
from datetime import datetime
from multiprocessing import Pool, cpu_count
from rich.progress import (
    Progress,
    TaskID,
    BarColumn,
    TextColumn,
    TimeRemainingColumn,
    SpinnerColumn,
    MofNCompleteColumn,
)
from rich.console import Console

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.cuda.amp import GradScaler
from loguru import logger
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

from loss import nllloss

# Richæ§åˆ¶å°
console = Console()


def train_client_worker(args_tuple):
    """
    å•ä¸ªå®¢æˆ·ç«¯è®­ç»ƒçš„å·¥ä½œå‡½æ•°ï¼ˆç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œï¼‰

    è¿™ä¸ªå‡½æ•°è¢«è®¾è®¡ä¸ºå¯ä»¥åœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­è¿è¡Œï¼Œå®ç°å®¢æˆ·ç«¯å¹¶è¡Œè®­ç»ƒ

    Args:
        args_tuple: åŒ…å«è®­ç»ƒå‚æ•°çš„å…ƒç»„
            - state_dict: æ¨¡å‹çŠ¶æ€å­—å…¸
            - dataloader_idx: æ•°æ®åŠ è½½å™¨çš„ç´¢å¼•
            - args: è®­ç»ƒé…ç½®
            - client_idx: å®¢æˆ·ç«¯ç´¢å¼•
            - train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨å­—å…¸

    Returns:
        tuple: (å®¢æˆ·ç«¯æ¨¡å‹çŠ¶æ€å­—å…¸, å¹³å‡æŸå¤±)
    """
    state_dict, dataloader_idx, args, client_idx, train_loader = args_tuple

    # ä»çŠ¶æ€å­—å…¸é‡å»ºæ¨¡å‹ï¼ˆéœ€è¦å¯¼å…¥æ¨¡å‹ç±»ï¼‰
    from backbone.BaseTransformer import BASE_Transformer

    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½å…¨å±€æ¨¡å‹å‚æ•°
    client_model = BASE_Transformer(
        input_nc=3,
        output_nc=2,
        token_len=4,
        resnet_stages_num=4,
        with_pos="learned",
        enc_depth=1,
        dec_depth=8,
    )
    client_model.load_state_dict(state_dict)
    client_model.to(args.device)
    client_model.train()

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        client_model.parameters(),
        lr=args.lr,
        betas=args.betas,
        eps=args.eps,
        weight_decay=args.weight_decay,
    )

    # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
    client_scaler = GradScaler()

    # è·å–å½“å‰å®¢æˆ·ç«¯çš„æ•°æ®åŠ è½½å™¨
    dataloader = train_loader[dataloader_idx]

    total_loss = 0.0
    num_batches = 0
    total_batches = len(dataloader) * args.num_client_epoch

    # åœ¨å®¢æˆ·ç«¯ä¸Šè¿›è¡Œå¤šä¸ªepochçš„æœ¬åœ°è®­ç»ƒ
    for epoch in range(args.num_client_epoch):
        for batch_idx, (A, B, Label, _) in enumerate(dataloader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶ç¡®ä¿å†…å­˜è¿ç»­
            A = A.contiguous().to(args.device, non_blocking=True)
            B = B.contiguous().to(args.device, non_blocking=True)
            Label = Label.contiguous().to(args.device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
            with torch.autocast(device_type=args.device, dtype=torch.float16):
                pred = client_model(A, B)
                loss = nllloss(pred[0].contiguous(), Label)

            client_scaler.scale(loss).backward()
            client_scaler.step(optimizer)
            client_scaler.update()

            total_loss += loss.item()
            num_batches += 1

    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / (num_batches * args.num_client_epoch)

    # è¿”å›æ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆä¸éœ€è¦è¿”å›æ•´ä¸ªæ¨¡å‹ï¼Œåªè¿”å›å‚æ•°ï¼‰
    return client_model.state_dict(), avg_loss


class FedTrain:
    """
    è”é‚¦å­¦ä¹ è®­ç»ƒç±»

    å®ç°è”é‚¦å­¦ä¹ çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒï¼ˆæ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œï¼‰
    - æ¨¡å‹æƒé‡èšåˆï¼ˆFedAvgï¼‰
    - å…¨å±€æ¨¡å‹è¯„ä¼°ï¼ˆåŒ…å«è¯¦ç»†æŒ‡æ ‡å’Œå¯è§†åŒ–ï¼‰
    - æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
    """

    def __init__(self, args, model, train_loader: list, test_loader: dict, n_clients: int):
        """
        åˆå§‹åŒ–è”é‚¦å­¦ä¹ è®­ç»ƒå™¨

        Args:
            args: è®­ç»ƒé…ç½®å‚æ•°
            model: å…¨å±€æ¨¡å‹
            train_loader: å„å®¢æˆ·ç«¯è®­ç»ƒæ•°æ®åŠ è½½å™¨åˆ—è¡¨ [dataloader0, dataloader1, ...]
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨å­—å…¸ {dataset_name: dataloader}
            n_clients: å®¢æˆ·ç«¯æ€»æ•°
        """
        self.args = args
        self.model = model.to(args.device)
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.args.n_clients = n_clients

        # ä½¿ç”¨DataParallelè¿›è¡Œå¤šGPUå¹¶è¡ŒåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.device_count() > 1 and not args.device.startswith("cpu"):
            logger.info(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
            self.model = nn.DataParallel(self.model)

        # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler()

        # åˆ›å»ºä¿å­˜æ¨¡å‹å’Œç»“æœçš„ç›®å½•
        self.save_dir = os.path.join(args.save_dir, f"fed_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        os.makedirs(self.save_dir, exist_ok=True)
        logger.info(f"æ¨¡å‹å’Œç»“æœå°†ä¿å­˜åˆ°: {self.save_dir}")

        # åˆå§‹åŒ–wandbï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import wandb
            self.wandb = wandb
            # å°è¯•è·å–å½“å‰çš„wandb run
            if wandb.run is not None:
                self.wandb_run = wandb.run
            else:
                self.wandb_run = None
            logger.info("WandBå·²åˆå§‹åŒ–")
        except ImportError:
            self.wandb = None
            self.wandb_run = None
            logger.warning("WandBæœªå®‰è£…ï¼Œå°†è·³è¿‡æ—¥å¿—è®°å½•")

        # è®°å½•æ¨¡å‹å‚æ•°åˆ°wandb
        if self.wandb is not None:
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            self.wandb.config.update({
                "model_total_params": total_params,
                "model_trainable_params": trainable_params,
                "num_gpus": torch.cuda.device_count() if torch.cuda.is_available() else 0,
                "device": args.device,
            })

    def train_client(self, model, dataloader, client_idx, progress=None):
        """
        åœ¨å•ä¸ªå®¢æˆ·ç«¯ä¸Šè¿›è¡Œæœ¬åœ°è®­ç»ƒï¼ˆå•è¿›ç¨‹ç‰ˆæœ¬ï¼‰

        æ˜¾ç¤ºè¯¦ç»†çš„è®­ç»ƒè¿›åº¦ï¼ŒåŒ…æ‹¬epochçº§åˆ«å’Œbatchçº§åˆ«çš„è¿›åº¦

        Args:
            model: å®¢æˆ·ç«¯åˆå§‹æ¨¡å‹ï¼ˆå…¨å±€æ¨¡å‹çš„å‰¯æœ¬ï¼‰
            dataloader: å®¢æˆ·ç«¯è®­ç»ƒæ•°æ®åŠ è½½å™¨
            client_idx: å®¢æˆ·ç«¯ç´¢å¼•
            progress: Rich Progresså¯¹è±¡ï¼ˆå¯é€‰ï¼‰

        Returns:
            tuple: (è®­ç»ƒåçš„æ¨¡å‹, å¹³å‡æŸå¤±)
        """
        # æ·±æ‹·è´æ¨¡å‹ï¼Œé¿å…å½±å“å…¨å±€æ¨¡å‹
        client_model = copy.deepcopy(model)
        client_model.train()

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            client_model.parameters(),
            lr=self.args.lr,
            betas=self.args.betas,
            eps=self.args.eps,
            weight_decay=self.args.weight_decay,
        )

        # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
        client_scaler = GradScaler()

        total_loss = 0.0
        num_batches = 0
        total_batches = len(dataloader) * self.args.num_client_epoch

        # åœ¨å®¢æˆ·ç«¯ä¸Šè¿›è¡Œå¤šä¸ªepochçš„æœ¬åœ°è®­ç»ƒ
        for epoch in range(self.args.num_client_epoch):
            epoch_start_time = time.time()
            epoch_loss = 0.0
            epoch_batches = 0
            epoch_task = None

            # åˆ›å»ºepochçº§åˆ«çš„è¿›åº¦æ¡
            if progress is not None:
                epoch_task = progress.add_task(
                    f"[cyan]å®¢æˆ·ç«¯ {client_idx} - Epoch {epoch + 1}/{self.args.num_client_epoch}",
                    total=len(dataloader)
                )
                iterator = dataloader
            else:
                # ä½¿ç”¨tqdm
                from tqdm import tqdm
                iterator = tqdm(dataloader, desc=f"å®¢æˆ·ç«¯ {client_idx} - Epoch {epoch + 1}/{self.args.num_client_epoch}")

            for batch_idx, (A, B, Label, _) in enumerate(iterator):
                A = A.contiguous().to(self.args.device, non_blocking=True)
                B = B.contiguous().to(self.args.device, non_blocking=True)
                Label = Label.contiguous().to(self.args.device, non_blocking=True)

                optimizer.zero_grad(set_to_none=True)

                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
                with torch.autocast(device_type=self.args.device, dtype=torch.float16):
                    pred = client_model(A, B)
                    loss = nllloss(pred[0].contiguous(), Label)

                client_scaler.scale(loss).backward()
                client_scaler.step(optimizer)
                client_scaler.update()

                total_loss += loss.item()
                epoch_loss += loss.item()
                num_batches += 1
                epoch_batches += 1

                # æ›´æ–°Richè¿›åº¦æ¡
                if progress is not None and epoch_task is not None:
                    progress.update(epoch_task, advance=1,
                                 description=f"[cyan]å®¢æˆ·ç«¯ {client_idx} - Epoch {epoch + 1}/{self.args.num_client_epoch} - Loss: {loss.item():.4f}")

            epoch_time = time.time() - epoch_start_time
            avg_epoch_loss = epoch_loss / epoch_batches if epoch_batches > 0 else 0.0

            logger.info(f"    å®¢æˆ·ç«¯ {client_idx} - Epoch {epoch + 1}/{self.args.num_client_epoch} å®Œæˆï¼ŒæŸå¤±: {avg_epoch_loss:.4f}, è€—æ—¶: {epoch_time:.2f}ç§’")
 
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / (num_batches * self.args.num_client_epoch)
 
        return client_model, avg_loss

    def train_clients_parallel(self, selected_client_indices, progress=None):
        """
        ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒå¤šä¸ªå®¢æˆ·ç«¯

        å¤šè¿›ç¨‹å¹¶è¡Œå¯ä»¥æ˜¾è‘—æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®¢æˆ·ç«¯æ•°é‡è¾ƒå¤šæ—¶

        Args:
            selected_client_indices: é€‰ä¸­çš„å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
            progress: Rich Progresså¯¹è±¡ï¼ˆå¯é€‰ï¼‰

        Returns:
            tuple: (å®¢æˆ·ç«¯æ¨¡å‹çŠ¶æ€å­—å…¸åˆ—è¡¨, å®¢æˆ·ç«¯æŸå¤±åˆ—è¡¨)
        """
        import multiprocessing as mp

        # è·å–å…¨å±€æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        global_state_dict = self.model.state_dict()

        # å‡†å¤‡æ¯ä¸ªå®¢æˆ·ç«¯çš„å‚æ•°
        client_args = []
        for idx in selected_client_indices:
            client_args.append((
                copy.deepcopy(global_state_dict),
                idx,
                self.args,
                idx,
                self.train_loader,
            ))

        # ç¡®å®šä½¿ç”¨çš„è¿›ç¨‹æ•°
        n_workers = min(self.args.n_workers if hasattr(self.args, 'n_workers') else cpu_count(), len(selected_client_indices))
        logger.info(f"ä½¿ç”¨ {n_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ {len(selected_client_indices)} ä¸ªå®¢æˆ·ç«¯")

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œè®­ç»ƒå®¢æˆ·ç«¯
        # æ³¨æ„ï¼šåœ¨Linux/WSLä¸Šä½¿ç”¨CUDAéœ€è¦ä½¿ç”¨'spawn' start method
        ctx = mp.get_context('spawn')
        client_models = []
        client_losses = []

        with ctx.Pool(processes=n_workers) as pool:
            # å¦‚æœæœ‰è¿›åº¦æ¡å¯¹è±¡ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨tqdm
            if progress is not None:
                # ä½¿ç”¨å¤–éƒ¨è¿›åº¦æ¡
                task = progress.add_task(
                    "[cyan]å¹¶è¡Œè®­ç»ƒå®¢æˆ·ç«¯ä¸­...", total=len(client_args)
                )
                results = []
                for result in pool.imap(train_client_worker, client_args):
                    results.append(result)
                    progress.update(task, advance=1)
            else:
                # ä½¿ç”¨tqdmï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
                from tqdm import tqdm
                results = list(tqdm(pool.imap(train_client_worker, client_args), total=len(client_args), desc="å¹¶è¡Œè®­ç»ƒå®¢æˆ·ç«¯"))

        for i, (state_dict, loss) in enumerate(results):
            client_models.append(state_dict)
            client_losses.append(loss)
            logger.info(f"  å®¢æˆ·ç«¯ {selected_client_indices[i]} è®­ç»ƒæŸå¤±: {loss:.4f}")

        return client_models, client_losses

    def average_weights(self, clients_model: list, client_weights=None):
        """
        ä½¿ç”¨FedAvgç®—æ³•èšåˆå®¢æˆ·ç«¯æ¨¡å‹æƒé‡

        Args:
            clients_model: å®¢æˆ·ç«¯æ¨¡å‹çŠ¶æ€å­—å…¸åˆ—è¡¨ [state_dict1, state_dict2, ...]
            client_weights: å®¢æˆ·ç«¯æƒé‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å¹³å‡æƒé‡

        Returns:
            dict: èšåˆåçš„å…¨å±€æ¨¡å‹çŠ¶æ€å­—å…¸
        """
        if not clients_model:
            logger.warning("æ²¡æœ‰å®¢æˆ·ç«¯æ¨¡å‹éœ€è¦èšåˆ")
            return self.model.state_dict()

        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æƒé‡
        if client_weights is None:
            client_weights = [1.0 / len(clients_model)] * len(clients_model)
        else:
            total_weight = sum(client_weights)
            client_weights = [w / total_weight for w in client_weights]

        # åˆå§‹åŒ–èšåˆåçš„æƒé‡å­—å…¸
        avg_weights = clients_model[0].copy()

        # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡ŒåŠ æƒå¹³å‡
        for key in avg_weights.keys():
            avg_weights[key] = avg_weights[key] * client_weights[0]

            for i in range(1, len(clients_model)):
                avg_weights[key] += clients_model[i][key] * client_weights[i]

        return avg_weights

    def calculate_iou(self, pred, target, num_classes=2):
        """
        è®¡ç®—æ¯ä¸ªç±»åˆ«çš„IoUï¼ˆIntersection over Unionï¼‰

        Args:
            pred: é¢„æµ‹ç»“æœ (N, H, W)
            target: çœŸå®æ ‡ç­¾ (N, H, W)
            num_classes: ç±»åˆ«æ•°é‡

        Returns:
            list: æ¯ä¸ªç±»åˆ«çš„IoUå€¼
        """
        ious = []
        valid_mask = target != 255
        pred = pred[valid_mask]
        target = target[valid_mask]

        for cls in range(num_classes):
            pred_mask = pred == cls
            target_mask = target == cls

            intersection = (pred_mask & target_mask).sum()
            union = (pred_mask | target_mask).sum()

            if union == 0:
                iou = 0.0
            else:
                iou = intersection / union
            ious.append(iou)

        return ious

    def plot_confusion_matrix(self, cm, classes, title='æ··æ·†çŸ©é˜µ'):
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜
        """
        plt.figure(figsize=(8, 6))
        plt.imshow(cm, interpolation='nearest', cmap='Blues')
        plt.title(title)
        plt.colorbar()

        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)

        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                plt.text(j, i, format(cm[i, j], 'd'),
                        horizontalalignment="center",
                        color="white" if cm[i, j] > thresh else "black")

        plt.tight_layout()
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')

        save_path = os.path.join(self.save_dir, f"{title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")

    def save_predictions(self, A, B, pred_mask, label, idx, ds_name):
        """
        ä¿å­˜é¢„æµ‹ç»“æœçš„å¯è§†åŒ–å›¾åƒ
        """
        A_np = A.cpu().numpy().transpose(1, 2, 0)
        B_np = B.cpu().numpy().transpose(1, 2, 0)

        if A_np.max() <= 1.0:
            A_np = (A_np * 255).astype(np.uint8)
        if B_np.max() <= 1.0:
            B_np = (B_np * 255).astype(np.uint8)

        pred_mask_np = pred_mask.cpu().numpy()
        pred_vis = np.zeros((pred_mask_np.shape[0], pred_mask_np.shape[1], 3), dtype=np.uint8)
        pred_vis[pred_mask_np == 1] = [255, 0, 0]

        label_np = label.cpu().numpy()
        label_vis = np.zeros((label_np.shape[0], label_np.shape[1], 3), dtype=np.uint8)
        label_vis[label_np == 1] = [0, 255, 0]

        vis = np.concatenate([A_np, B_np, pred_vis, label_vis], axis=1)

        save_path = os.path.join(self.save_dir, f"{ds_name}_prediction_{idx}.png")
        plt.imsave(save_path, vis)

    def evaluate_model(self, model, test_loader, ds_name, save_samples=True, progress=None):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆåŒ…å«è¯¦ç»†æŒ‡æ ‡å’Œå¯è§†åŒ–ï¼‰

        Args:
            model: è¦è¯„ä¼°çš„æ¨¡å‹
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            ds_name: æ•°æ®é›†åç§°
            save_samples: æ˜¯å¦ä¿å­˜é¢„æµ‹æ ·æœ¬
            progress: Rich Progresså¯¹è±¡ï¼ˆå¯é€‰ï¼‰
        """
        model.eval()

        all_preds = []
        all_labels = []
        total_loss = 0.0
        num_samples = 0

        inference_times = []

        # ä½¿ç”¨Richè¿›åº¦æ¡æ˜¾ç¤ºæµ‹è¯•è¿›åº¦ï¼ˆå¦‚æœæä¾›äº†ï¼‰
        task = None
        if progress is not None:
            task = progress.add_task(
                f"[cyan]æµ‹è¯• {ds_name} ä¸­...", total=len(test_loader)
            )
            iterator = test_loader
        else:
            # ä½¿ç”¨tqdmï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
            from tqdm import tqdm
            iterator = tqdm(test_loader, desc=f"æµ‹è¯• {ds_name}")

        with torch.no_grad():
            for batch_idx, (A, B, Label, _) in enumerate(iterator):
                A = A.contiguous().to(self.args.device, non_blocking=True)
                B = B.contiguous().to(self.args.device, non_blocking=True)
                Label = Label.contiguous().to(self.args.device, non_blocking=True)

                start_time = time.time()

                with torch.autocast(device_type=self.args.device, dtype=torch.float16):
                    pred = model(A, B)
                    loss = nllloss(pred[0].contiguous(), Label)

                inference_time = time.time() - start_time
                inference_times.append(inference_time)

                total_loss += loss.item() * A.size(0)
                num_samples += A.size(0)

                pred_mask = torch.argmax(pred[0], dim=1)

                all_preds.append(pred_mask.cpu().numpy())
                all_labels.append(Label.cpu().numpy())

                if save_samples and batch_idx < 5:
                    for i in range(min(A.size(0), 2)):
                        self.save_predictions(
                            A[i], B[i], pred_mask[i], Label[i],
                            batch_idx * test_loader.batch_size + i, ds_name
                        )

                # æ›´æ–°è¿›åº¦æ¡
                if progress is not None and task is not None:
                    progress.update(task, advance=1)

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / num_samples

        # åˆå¹¶æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾
        all_preds = np.concatenate(all_preds).flatten()
        all_labels = np.concatenate(all_labels).flatten()

        # è¿‡æ»¤æ‰ignore_index
        valid_mask = all_labels != 255
        all_preds = all_preds[valid_mask]
        all_labels = all_labels[valid_mask]

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        accuracy = (all_preds == all_labels).mean()

        precision = precision_score(all_labels, all_preds, zero_division='0')

        recall = recall_score(all_labels, all_preds, zero_division='0')

        f1 = f1_score(all_labels, all_preds, zero_division='0')

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])
        self.plot_confusion_matrix(cm, classes=['æœªå˜åŒ–', 'å˜åŒ–'], title=f'{ds_name}_æ··æ·†çŸ©é˜µ')

        # è®¡ç®—IoU
        ious = self.calculate_iou(all_preds, all_labels, num_classes=2)
        miou = np.mean(ious)

        # è®¡ç®—æ¨ç†é€Ÿåº¦
        avg_inference_time = np.mean(inference_times) * 1000
        fps = 1000 / avg_inference_time if avg_inference_time > 0 else 0

        metrics = {
            "loss": avg_loss,
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "iou_unchanged": ious[0],
            "iou_changed": ious[1],
            "miou": miou,
            "inference_time_ms": avg_inference_time,
            "fps": fps,
        }

        # è®°å½•æµ‹è¯•æŒ‡æ ‡åˆ°wandb
        if self.wandb is not None:
            import wandb
            prefix = f"test/{ds_name}"
            self.wandb.log({
                f"{prefix}/loss": avg_loss,
                f"{prefix}/accuracy": accuracy,
                f"{prefix}/precision": precision,
                f"{prefix}/recall": recall,
                f"{prefix}/f1": f1,
                f"{prefix}/iou_unchanged": ious[0],
                f"{prefix}/iou_changed": ious[1],
                f"{prefix}/miou": miou,
                f"{prefix}/inference_time_ms": avg_inference_time,
                f"{prefix}/fps": fps,
            }, step=getattr(self, 'current_round', 0))

            # ä¸Šä¼ æ··æ·†çŸ©é˜µåˆ°wandb
            cm_path = os.path.join(self.save_dir, f"{ds_name}_æ··æ·†çŸ©é˜µ_*.png")
            import glob
            cm_files = glob.glob(cm_path)
            if cm_files:
                self.wandb.log({
                    f"{prefix}/confusion_matrix": wandb.Image(cm_files[-1])
                }, step=getattr(self, 'current_round', 0))

            # ä¸Šä¼ é¢„æµ‹ç»“æœç¤ºä¾‹
            pred_path = os.path.join(self.save_dir, f"{ds_name}_prediction_*.png")
            pred_files = sorted(glob.glob(pred_path))[:5]  # åªä¸Šä¼ å‰5å¼ 
            for i, pred_file in enumerate(pred_files):
                self.wandb.log({
                    f"{prefix}/prediction_{i}": wandb.Image(pred_file)
                }, step=getattr(self, 'current_round', 0))

        return metrics

    def save_model(self, model, epoch, is_best=False):
        """
        ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'config': vars(self.args),
        }

        save_path = os.path.join(self.save_dir, f"model_epoch_{epoch}.pth")
        torch.save(checkpoint, save_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

        # è®°å½•æ¨¡å‹åˆ°wandb
        if self.wandb is not None:
            import wandb
            self.wandb.save(save_path, base_path=self.save_dir)

        if is_best:
            best_path = os.path.join(self.save_dir, "model_best.pth")
            torch.save(checkpoint, best_path)
            logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_path}")

            # è®°å½•æœ€ä½³æ¨¡å‹åˆ°wandb
            if self.wandb is not None:
                self.wandb.save(best_path, base_path=self.save_dir)

    def load_model(self, checkpoint_path):
        """
        ä»æ–‡ä»¶åŠ è½½æ¨¡å‹
        """
        if not os.path.exists(checkpoint_path):
            logger.warning(f"checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0

        checkpoint = torch.load(checkpoint_path, map_location=self.args.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])

        epoch = checkpoint.get('epoch', 0)
        logger.info(f"å·²ä» {checkpoint_path} åŠ è½½æ¨¡å‹ï¼Œepoch: {epoch}")

        return epoch

    def test(self, progress=None):
        """
        åœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šè¯„ä¼°å…¨å±€æ¨¡å‹æ€§èƒ½

        Args:
            progress: Rich Progresså¯¹è±¡ï¼ˆå¯é€‰ï¼‰
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•å…¨å±€æ¨¡å‹...")
        logger.info("=" * 60)

        all_metrics = []

        for ds_name, test_loader in self.test_loader.items():
            logger.info(f"\næ­£åœ¨è¯„ä¼° {ds_name} æµ‹è¯•é›†...")

            metrics = self.evaluate_model(self.model, test_loader, ds_name, save_samples=True, progress=progress)
            all_metrics.append(metrics)

            logger.info(f"\n{ds_name} æµ‹è¯•ç»“æœ:")
            logger.info(f"  - æŸå¤±: {metrics['loss']:.4f}")
            logger.info(f"  - å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
            logger.info(f"  - ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
            logger.info(f"  - å¬å›ç‡: {metrics['recall']:.4f}")
            logger.info(f"  - F1åˆ†æ•°: {metrics['f1']:.4f}")
            logger.info(f"  - IoU (æœªå˜åŒ–): {metrics['iou_unchanged']:.4f}")
            logger.info(f"  - IoU (å˜åŒ–): {metrics['iou_changed']:.4f}")
            logger.info(f"  - å¹³å‡IoU: {metrics['miou']:.4f}")
            logger.info(f"  - æ¨ç†æ—¶é—´: {metrics['inference_time_ms']:.2f} ms/å¼ ")
            logger.info(f"  - æ¨ç†é€Ÿåº¦: {metrics['fps']:.2f} FPS")

        # è®¡ç®—å¹¶è¾“å‡ºå¹³å‡æŒ‡æ ‡
        avg_metrics = {
            "loss": np.mean([m["loss"] for m in all_metrics]),
            "accuracy": np.mean([m["accuracy"] for m in all_metrics]),
            "precision": np.mean([m["precision"] for m in all_metrics]),
            "recall": np.mean([m["recall"] for m in all_metrics]),
            "f1": np.mean([m["f1"] for m in all_metrics]),
            "miou": np.mean([m["miou"] for m in all_metrics]),
            "inference_time_ms": np.mean([m["inference_time_ms"] for m in all_metrics]),
            "fps": np.mean([m["fps"] for m in all_metrics]),
        }

        logger.info("\n" + "=" * 60)
        logger.info("æ‰€æœ‰æµ‹è¯•é›†å¹³å‡ç»“æœ:")
        logger.info(f"  - å¹³å‡æŸå¤±: {avg_metrics['loss']:.4f}")
        logger.info(f"  - å¹³å‡å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f}")
        logger.info(f"  - å¹³å‡ç²¾ç¡®ç‡: {avg_metrics['precision']:.4f}")
        logger.info(f"  - å¹³å‡å¬å›ç‡: {avg_metrics['recall']:.4f}")
        logger.info(f"  - å¹³å‡F1åˆ†æ•°: {avg_metrics['f1']:.4f}")
        logger.info(f"  - å¹³å‡IoU: {avg_metrics['miou']:.4f}")
        logger.info(f"  - å¹³å‡æ¨ç†æ—¶é—´: {avg_metrics['inference_time_ms']:.2f} ms/å¼ ")
        logger.info(f"  - å¹³å‡æ¨ç†é€Ÿåº¦: {avg_metrics['fps']:.2f} FPS")
        logger.info("=" * 60)

        return avg_metrics

    def start_train(self):
        """
        å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒæµç¨‹
        """
        # ä½¿ç”¨Richæ˜¾ç¤ºè®­ç»ƒé…ç½®
        console.print("\n[bold blue]è®­ç»ƒé…ç½®[/bold blue]")
        console.print(f"  å®¢æˆ·ç«¯æ€»æ•°: [cyan]{self.args.n_clients}[/cyan]")
        console.print(f"  æ¯è½®å‚ä¸å®¢æˆ·ç«¯æ¯”ä¾‹: [cyan]{self.args.frac}[/cyan]")
        console.print(f"  è®­ç»ƒè½®æ•°: [cyan]{self.args.num_epochs}[/cyan]")
        console.print(f"  å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒè½®æ•°: [cyan]{self.args.num_client_epoch}[/cyan]")
        console.print(f"  è¯„ä¼°é—´éš”: [cyan]æ¯ {self.args.eval_interval} è½®è¯„ä¼°ä¸€æ¬¡[/cyan]")
        console.print(f"  ä½¿ç”¨å¹¶è¡Œè®­ç»ƒ: [cyan]{getattr(self.args, 'use_parallel', True)}[/cyan]")

        train_losses = []
        best_f1 = 0.0

        # ä½¿ç”¨Richè¿›åº¦æ¡æ˜¾ç¤ºæ•´ä½“è®­ç»ƒè¿›åº¦
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            TimeRemainingColumn(),
            console=console,
        ) as progress:

            # åˆ›å»ºæ€»ä½“è®­ç»ƒä»»åŠ¡
            overall_task = progress.add_task(
                "[bold green]è”é‚¦å­¦ä¹ è®­ç»ƒè¿›åº¦", total=self.args.num_epochs
            )

            for round_idx in range(self.args.num_epochs):
                self.current_round = round_idx  # ç”¨äºwandbæ—¥å¿—è®°å½•
                round_start_time = time.time()

                progress.console.print(f"\n[bold cyan]{'=' * 60}[/bold cyan]")
                progress.console.print(f"[bold cyan]è®­ç»ƒè½®æ¬¡: {round_idx + 1}/{self.args.num_epochs}[/bold cyan]")
                progress.console.print(f"[bold cyan]{'=' * 60}[/bold cyan]")

                # éšæœºé€‰æ‹©å‚ä¸æœ¬è½®è®­ç»ƒçš„å®¢æˆ·ç«¯
                m = max(int(self.args.frac * self.args.n_clients), 1)
                selected_client_indices = np.random.choice(
                    range(self.args.n_clients), m, replace=False
                )

                logger.info(f"æœ¬è½®é€‰ä¸­çš„å®¢æˆ·ç«¯: {selected_client_indices.tolist()}")

                # è®°å½•è®­ç»ƒé…ç½®åˆ°wandb
                if self.wandb is not None and round_idx == 0:
                    self.wandb.config.update({
                        "selected_clients_per_round": m,
                        "total_clients": self.args.n_clients,
                        "client_fraction": self.args.frac,
                    })

                client_models = []
                client_losses = []

                use_parallel = getattr(self.args, 'use_parallel', True)

                if use_parallel:
                    client_models, client_losses = self.train_clients_parallel(selected_client_indices, progress)
                else:
                    for client_idx in selected_client_indices:
                        logger.info(f"  è®­ç»ƒå®¢æˆ·ç«¯ {client_idx}...")

                        client_model, client_loss = self.train_client(
                            model=self.model,
                            dataloader=self.train_loader[client_idx],
                            client_idx=client_idx,
                            progress=progress,
                        )

                        client_models.append(client_model.state_dict())
                        client_losses.append(client_loss)

                        logger.info(f"  å®¢æˆ·ç«¯ {client_idx} è®­ç»ƒæŸå¤±: {client_loss:.4f}")

                        # è®°å½•å®¢æˆ·ç«¯æŸå¤±åˆ°wandb
                        if self.wandb is not None:
                            self.wandb.log({
                                f"train/round_{round_idx}/client_{client_idx}_loss": client_loss,
                            }, step=round_idx)

                # èšåˆå®¢æˆ·ç«¯æ¨¡å‹å‚æ•°
                updated_weights = self.average_weights(client_models)
                self.model.load_state_dict(updated_weights)

                # è®¡ç®—æœ¬è½®å¹³å‡æŸå¤±
                round_avg_loss = sum(client_losses) / len(client_losses)
                train_losses.append(round_avg_loss)

                round_time = time.time() - round_start_time

                # è®°å½•è½®æ¬¡çº§åˆ«æŒ‡æ ‡åˆ°wandb
                if self.wandb is not None:
                    import wandb
                    self.wandb.log({
                        "train/round_loss": round_avg_loss,
                        "train/round_time": round_time,
                        "train/clients_per_second": m / round_time,
                        "train/selected_clients": selected_client_indices.tolist(),
                    }, step=round_idx)

                # ä½¿ç”¨Richæ˜¾ç¤ºæœ¬è½®è®­ç»ƒç»“æœ
                progress.console.print(f"\n[bold yellow]è½®æ¬¡ {round_idx + 1} æ€»ç»“:[/bold yellow]")
                progress.console.print(f"  - å¹³å‡è®­ç»ƒæŸå¤±: [red]{round_avg_loss:.4f}[/red]")
                progress.console.print(f"  - æœ¬è½®è€—æ—¶: [cyan]{round_time:.2f}[/cyan] ç§’")
                progress.console.print(f"  - è®­ç»ƒé€Ÿåº¦: [cyan]{m / round_time:.2f}[/cyan] å®¢æˆ·ç«¯/ç§’")

                # æ›´æ–°æ€»ä½“è¿›åº¦
                progress.update(overall_task, advance=1)

                # å®šæœŸè¯„ä¼°æ¨¡å‹
                if round_idx % self.args.eval_interval == 0:
                    progress.console.print(f"\n[bold magenta]å¼€å§‹è¯„ä¼°æ¨¡å‹ï¼ˆç¬¬ {round_idx + 1} è½®ï¼‰...[/bold magenta]")
                    test_metrics = self.test(progress=progress)

                    # è®°å½•æµ‹è¯•æŒ‡æ ‡åˆ°wandb
                    if self.wandb is not None:
                        import wandb
                        for ds_name, ds_metrics in zip(self.test_loader.keys(), [test_metrics]):
                            self.wandb.log({
                                f"val/{ds_name}/loss": ds_metrics.get('loss', 0),
                                f"val/{ds_name}/accuracy": ds_metrics.get('accuracy', 0),
                                f"val/{ds_name}/f1": ds_metrics.get('f1', 0),
                                f"val/{ds_name}/miou": ds_metrics.get('miou', 0),
                                f"val/{ds_name}/fps": ds_metrics.get('fps', 0),
                            }, step=round_idx)

                    if test_metrics['f1'] > best_f1:
                        best_f1 = test_metrics['f1']
                        progress.console.print(f"[bold green]ğŸ‰ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹ï¼F1åˆ†æ•°: {best_f1:.4f}[/bold green]")
                        self.save_model(self.model, round_idx, is_best=True)

                        # è®°å½•æœ€ä½³æŒ‡æ ‡åˆ°wandb
                        if self.wandb is not None:
                            import wandb
                            wandb.config.update({
                                "best_f1": best_f1,
                                "best_round": round_idx,
                            })

                    self.save_model(self.model, round_idx, is_best=False)

        logger.info("\n" + "=" * 60)
        logger.info("è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
        logger.info(f"æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
        logger.info("=" * 60)

        # æœ€ç»ˆè¯„ä¼°
        logger.info("\næœ€ç»ˆè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        best_model_path = os.path.join(self.save_dir, "model_best.pth")
        self.load_model(best_model_path)
        final_metrics = self.test(progress=None)  # æœ€ç»ˆæµ‹è¯•ä¸éœ€è¦è¿›åº¦æ¡

        logger.info("\nè®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
        logger.info(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.save_dir}")


def main():
    """
    ä¸»å‡½æ•°ï¼šå¯åŠ¨è”é‚¦å­¦ä¹ è®­ç»ƒæµç¨‹
    """
    from datetime import datetime
    import wandb
    import torch

    from backbone.BaseTransformer import BASE_Transformer
    from utils.args import get_fed_config
    from assgin_ds import get_fed_dataloaders_with_allocator
    from loguru import logger

    wandb.login()

    project_name = "change-detection-demo"

    ds_name = {
        "LEVIR": {
            "path": "/home/dhm/dataset/LEVIR",
            "n_clients": 2,
            "data_ratios": [0.6, 0.4],
            "sampler_configs": [
                {"type": "random", "shuffle": True},
                {"type": "weighted", "shuffle": True, "weights": None},
            ],
        },
        "S2Looking": {
            "path": "/home/dhm/dataset/S2Looking",
            "n_clients": 4,
            "data_ratios": [0.5, 0.3, 0.15, 0.05],
            "sampler_configs": [
                {"type": "random", "shuffle": True},
                {"type": "sequential"},
                {"type": "random", "shuffle": True},
                {"type": "weighted", "shuffle": True, "weights": None},
            ],
        },
        "WHUCD": {
            "path": "/home/dhm/dataset/WHUCD",
            "n_clients": 2,
            "data_ratios": [0.5, 0.5],
            "sampler_configs": [
                {"type": "random", "shuffle": True},
                {"type": "sequential"},
            ],
        },
    }

    if __name__ == "__main__":
        fed_config = get_fed_config()

        # å°†é…ç½®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        config_dict = vars(fed_config)

        with wandb.init(project=project_name, config=config_dict) as run:
            current_time = datetime.now()
            time_str = current_time.strftime("%Y%m%d_%H%M")

            print(f"\n{'=' * 60}")

            # ========== ç¬¬1æ­¥ï¼šåŠ è½½æ•°æ®é›† ==========
            console.print("[bold blue]æ­£åœ¨åŠ è½½æ•°æ®é›†...[/bold blue]")
            from assgin_ds import get_fed_dataset

            train_dict, test_dict = get_fed_dataset(
                args=fed_config, ds_name=ds_name
            )

            train_loaders, test_loaders, client_info = get_fed_dataloaders_with_allocator(
                train_datasets=train_dict,
                test_datasets=test_dict,
                ds_name=ds_name,
                args=fed_config,
            )

            console.print(f"\n[bold green]âœ… æ•°æ®åˆ†é…å®Œæˆï¼[/bold green]")
            console.print(f"æ€»å®¢æˆ·ç«¯æ•°: [cyan]{len(train_loaders)}[/cyan]")
            console.print(f"æµ‹è¯•æ•°æ®é›†æ•°: [cyan]{len(test_loaders)}[/cyan]")

            tot_client = 0
            current_client_id = 0

            for ds_name, ds_info in ds_name.items():
                n_clients = ds_info["n_clients"]
                tot_client += n_clients
                current_client_id += n_clients

            # ========== ç¬¬2æ­¥ï¼šåˆå§‹åŒ–æ¨¡å‹ ==========
            console.print("\n[bold blue]æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...[/bold blue]")

            model = BASE_Transformer(
                input_nc=3,
                output_nc=2,
                token_len=4,
                resnet_stages_num=4,
                with_pos="learned",
                enc_depth=1,
                dec_depth=8,
            )

            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            console.print(f"  - æ€»å‚æ•°é‡: [cyan]{total_params:,}[/cyan]")
            console.print(f"  - å¯è®­ç»ƒå‚æ•°é‡: [cyan]{trainable_params:,}[/cyan]")
            console.print(f"[bold green]âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼[/bold green]\n")

            # ========== ç¬¬3æ­¥ï¼šå¯åŠ¨è”é‚¦å­¦ä¹ è®­ç»ƒ ==========
            logger.info(f"å®¢æˆ·ç«¯æ•°é‡: {tot_client}")

            Trainer = FedTrain(
                args=fed_config,
                model=model,
                train_loader=train_loaders,
                test_loader=test_dict,
                n_clients=tot_client
            )

            Trainer.start_train()

            console.print("\n[bold green]ğŸ‰ è®­ç»ƒå®Œæˆï¼[/bold green]")


if __name__ == "__main__":
    main()
